{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c3fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "308a9ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "25d9524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"Pippi is baking pepparkakor cookies when they arrive, so they watch and wait for her to finish.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cf43eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = nltk.sent_tokenize(sent1.replace('\\r', '').replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "36fd2b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pippi is baking pepparkakor cookies when they arrive, so they watch and wait for her to finish.']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "01f163ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pippi PROPN nsubj\n",
      "is AUX aux\n",
      "baking VERB ccomp\n",
      "pepparkakor NOUN compound\n",
      "cookies NOUN dobj\n",
      "when ADV advmod\n",
      "they PRON nsubj\n",
      "arrive VERB advcl\n",
      ", PUNCT punct\n",
      "so ADV advmod\n",
      "they PRON nsubj\n",
      "watch VERB ROOT\n",
      "and CCONJ cc\n",
      "wait VERB conj\n",
      "for ADP mark\n",
      "her PRON nsubj\n",
      "to PART aux\n",
      "finish NOUN advcl\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "dependency_parsing_2 = []\n",
    "for sent in sentence_tokens:\n",
    "    for token in nlp(sent):\n",
    "        print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f24dcb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer  = r\"\"\"\n",
    "PP: {<IN>(<NP>|<AdjP>|<NN*>)} \n",
    "AdjP: {(<PRP.*><JJ>|<DT><JJ>|<JJ>)<NN.*|NNP|PP>* }\n",
    "AdvP: {<RB.*><VB.*|JJ.*|NN.*>}\n",
    "NP: {<DT>*(<PRP.*>|<NN.*>|VBG)?<NN.*|PRP.*>}\n",
    "VP: {<MD>?(<VB.*>*|<AdvP>)}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f2273cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkParser = nltk.RegexpParser(grammer)\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(sent1))\n",
    "tree = chunkParser.parse(tagged)\n",
    "\n",
    "chunkedText = []\n",
    "for subtree in tree.subtrees():\n",
    "    chunkedText.append(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "146b00e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current item: ['NP', 'Pippi/NNP']\n",
      "Current item: ['VP', 'is/VBZ', 'baking/VBG']\n",
      "['VP', 'is/VBZ', 'baking/VBG'] ['AdjP', 'pepparkakor/JJ', 'cookies/NNS']\n",
      "Current item: ['AdjP', 'pepparkakor/JJ', 'cookies/NNS']\n",
      "Current item: ['NP', 'they/PRP']\n",
      "Current item: ['VP', 'arrive/VBP']\n",
      "['VP', 'arrive/VBP'] ['NP', 'they/PRP']\n",
      "Current item: ['NP', 'they/PRP']\n",
      "Current item: ['VP', 'watch/VBP']\n",
      "Current item: ['VP', 'wait/VBP']\n",
      "['VP', 'wait/VBP'] ['NP', 'her/PRP$']\n",
      "Current item: ['NP', 'her/PRP$']\n",
      "Current item: ['VP', 'finish/VB']\n"
     ]
    }
   ],
   "source": [
    "list_of_compounds = []\n",
    "for i in list(np.arange(1,len(chunkedText))):\n",
    "    curr_item = str(chunkedText[i]).replace('(','').replace(')','').split(' ')\n",
    "    print('Current item:',curr_item)\n",
    "    if (i< len(chunkedText) -2):\n",
    "        for j in list(np.arange(i+1,i+2)):\n",
    "            next_item = str(chunkedText[j]).replace('(','').replace(')','').split(' ')\n",
    "            if curr_item[0] == 'VP' and (next_item[0] == 'PP' or next_item[0] == 'AdjP' or next_item[0] == 'NP'):   \n",
    "                list_of_compounds.append(curr_item + next_item)\n",
    "                print(curr_item, next_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1cb77a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['VP', 'is/VBZ', 'baking/VBG', 'AdjP', 'pepparkakor/JJ', 'cookies/NNS'],\n",
       " ['VP', 'arrive/VBP', 'NP', 'they/PRP'],\n",
       " ['VP', 'wait/VBP', 'NP', 'her/PRP$']]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6c072c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\n",
      "-1\n",
      "-1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "['AdjP', 'pepparkakor/JJ', 'cookies/NNS'] ['VP', 'arrive/VBP']\n",
      "3\n",
      "4\n",
      "4\n",
      "['NP', 'they/PRP'] ['VP', 'watch/VBP']\n",
      "5\n",
      "5\n",
      "6\n",
      "['NP', 'they/PRP'] ['VP', 'wait/VBP']\n",
      "6\n",
      "7\n",
      "7\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['AdjP', 'pepparkakor/JJ', 'cookies/NNS', 'VP', 'arrive/VBP'],\n",
       " ['NP', 'they/PRP', 'VP', 'watch/VBP'],\n",
       " ['NP', 'they/PRP', 'VP', 'wait/VBP']]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Who questions?\n",
    "list_of_subjects = []\n",
    "for i in list(np.arange(1,len(chunkedText))):\n",
    "    curr_item = str(chunkedText[i]).replace('(','').replace(')','').split(' ')\n",
    "    for j in list(np.arange(i+,i+2)):\n",
    "        print(j)\n",
    "        prev_item = str(chunkedText[j]).replace('(','').replace(')','').split(' ')\n",
    "        if (curr_item[0] == 'NP' or curr_item[0] == 'AdjP')  and (prev_item[0] == 'NP' or prev_item[0] == 'AdjP'):   \n",
    "            list_of_subjects.append(prev_item + curr_item )\n",
    "            print(prev_item ,curr_item)\n",
    "\n",
    "list_of_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb5506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when where whom what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd9da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many events does aoccur in each chapter?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
